{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfeee5ab-7a13-451e-b9c7-812e2c73fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib as ta\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.simplefilter(\"ignore\")\n",
    "df=pd.read_html(\"https://zh.wikipedia.org/zh-cn/%E7%B4%8D%E6%96%AF%E9%81%94%E5%85%8B100%E6%8C%87%E6%95%B8\")\n",
    "stock_list=df[2].drop(columns=\"加入日期\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc66f93-2365-46f7-a2d0-77685f291648",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_index=stock_list[\"股票代号\"].tolist()\n",
    "for stock in stock_index:\n",
    "    data=yf.download(stock,start='2020-05-31',end=\"2024-12-31\",progress=False)\n",
    "    trading_dates=data.index.tolist()\n",
    "    data[\"Dates\"]=trading_dates\n",
    "    data=data[[\"Dates\",\"Close\",\"High\",\"Low\",\"Open\",\"Volume\"]]\n",
    "    #data=data.drop(data.index[0])\n",
    "    data=data.round(2)\n",
    "    data.to_csv(f\"{stock}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5267d29-eb35-4150-af85-97ca80abb3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock in stock_index:\n",
    "    data=pd.read_csv(f\"{stock}.csv\")\n",
    "    data=data.drop(data.index[0])\n",
    "    data.to_csv(f\"{stock}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5461903-9368-407f-98b2-9fae987cee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock in stock_index:\n",
    "    data=pd.read_csv(f\"{stock}.csv\")\n",
    "    data[\"RSI\"]=ta.RSI(data[\"Close\"])\n",
    "    data['MACD'], data['MACD_SIGNAL'], data['MACD_HIST'] = ta.MACD(\n",
    "    data['Close'],fastperiod=12,slowperiod=26,signalperiod=9)\n",
    "    data['PPO'] = ta.PPO(data['Close'],fastperiod=12,slowperiod=26,matype=0)\n",
    "    data['PPO_SIGNAL'] = ta.SMA(data['PPO'], timeperiod=9)\n",
    "    data['PPO_HIST'] = data['PPO'] - data['PPO_SIGNAL']\n",
    "    data['ADX_14'] = ta.ADX(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "    data['Momentum_10'] = ta.MOM(data['Close'], timeperiod=10)\n",
    "    data['CCI_14'] = ta.CCI(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "    data['ROC_10'] = ta.ROC(data['Close'], timeperiod=10)\n",
    "    data['STOCH_K'], data['STOCH_D'] = ta.STOCH(data['High'],data['Low'],data['Close'],\n",
    "    fastk_period=14,slowk_period=3,slowk_matype=0,slowd_period=3,slowd_matype=0)\n",
    "    data['WILLR_14'] = ta.WILLR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "    data['SMA_20'] = ta.SMA(data['Close'], timeperiod=20)\n",
    "    data['SMA_50'] = ta.SMA(data['Close'], timeperiod=50)\n",
    "    data['EMA_20'] = ta.EMA(data['Close'], timeperiod=20)\n",
    "    data['EMA_50'] = ta.EMA(data['Close'], timeperiod=50)\n",
    "    data['BB_upper'], data['BB_middle'], data['BB_lower'] = ta.BBANDS(\n",
    "    data['Close'],timeperiod=20,nbdevup=2,nbdevdn=2,matype=0)\n",
    "    data['OBV'] = ta.OBV(data['Close'], data['Volume'])\n",
    "    data['PSAR'] = ta.SAR(data['High'], data['Low'], acceleration=0.02, maximum=0.2)\n",
    "    data['Chaikin_Osc'] = ta.ADOSC(data['High'], data['Low'], data['Close'], data['Volume'], fastperiod=3, slowperiod=10)\n",
    "    data['MFI_14'] = ta.MFI(data['High'], data['Low'], data['Close'], data['Volume'], timeperiod=14)\n",
    "    data['ATR_14'] = ta.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "    data[\"Y\"]=np.where(data['Close'].shift(-1) - data['Close'] > 0, 1, -1)\n",
    "    data.to_csv(f\"{stock}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c5e7c9d-6f18-48fb-ba2d-79c2b6ba5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock in stock_index:\n",
    "    data=pd.read_csv(f\"{stock}.csv\")\n",
    "    data=data[data[\"Dates\"]>\"2020-12-31\"]\n",
    "    data=data.round(2)\n",
    "    data.to_csv(f\"{stock}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483ce360-3bd5-4579-b054-9359a6148584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABNB存在缺失值\n",
      "ABNB缺失值为 204\n",
      "ARM存在缺失值\n",
      "ARM缺失值为 542\n",
      "CEG存在缺失值\n",
      "CEG缺失值为 542\n",
      "DASH存在缺失值\n",
      "DASH缺失值为 188\n",
      "GEHC存在缺失值\n",
      "GEHC缺失值为 542\n",
      "GFS存在缺失值\n",
      "GFS缺失值为 542\n",
      "['ABNB', 'ARM', 'CEG', 'DASH', 'GEHC', 'GFS']\n"
     ]
    }
   ],
   "source": [
    "nan_list=[]\n",
    "for stock in stock_index:\n",
    "    data=pd.read_csv(f\"{stock}.csv\")\n",
    "    columns=data.columns.tolist()\n",
    "    sum=0\n",
    "    for column in columns:\n",
    "        sum+=data[column].isnull().sum()\n",
    "    if sum!=0:\n",
    "        print(f\"{stock}存在缺失值\" )\n",
    "        print(f\"{stock}缺失值为\",sum)\n",
    "        nan_list.append(stock)\n",
    "print(nan_list)\n",
    "for stock in nan_list:\n",
    "    stock_index.remove(stock)\n",
    "#stock_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f010af5-92dc-40e1-91fd-d2500a34cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "def data_preprocessing(stock):\n",
    "    data=pd.read_csv(f\"{stock}.csv\")\n",
    "    columns=data.columns.tolist()\n",
    "    columns.remove(\"Dates\")\n",
    "    target=data[\"Y\"]\n",
    "    columns.remove(\"Y\")\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[columns] = scaler.fit_transform(data[columns])\n",
    "    pca=PCA(n_components=0.95)\n",
    "    data_pca = pd.DataFrame(pca.fit_transform(data[columns]))\n",
    "    data_pca[\"target\"]=target\n",
    "    return data_pca.to_csv(f\"{stock}_pca.csv\",index=False)\n",
    "for stock in stock_index:\n",
    "    data_preprocessing(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56805a7f-324d-405f-a6f8-d248cd6da712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock: REGN\n",
      "Best Parameters: {'wavelet': 'sym20', 'xgb__n_estimators': 100, 'xgb__max_depth': 5, 'xgb__learning_rate': 0.1, 'xgb__subsample': 0.6, 'xgb__colsample_bytree': 0.6, 'xgb__gamma': 0.5}\n",
      "Best Accuracy (Validation): 0.62\n",
      "Test Accuracy: 0.6039603960396039\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 定义小波基\n",
    "wavelet_base = [\"haar\", \"db3\", \"db6\", \"db9\", \"db15\", \"db20\", \"sym3\", \"sym6\", \"sym9\", \"sym15\", \"sym20\"]\n",
    "\n",
    "# 定义降噪函数\n",
    "def denoise_signal(signal, wavelet, level):\n",
    "    \"\"\"\n",
    "    对单个信号应用DWT降噪。\n",
    "    \"\"\"\n",
    "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "    sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
    "    uthresh = sigma * np.sqrt(2 * np.log(len(signal)))\n",
    "    denoised_coeffs = [pywt.threshold(c, value=uthresh, mode='soft') for c in coeffs]\n",
    "    denoised_signal = pywt.waverec(denoised_coeffs, wavelet)\n",
    "    return denoised_signal[:len(signal)]  # 修正长度\n",
    "\n",
    "# 遍历股票列表\n",
    "for stock in list(np.random.choice(stock_index,size=1)):\n",
    "    # 读取 PCA 数据\n",
    "    pca_data = pd.read_csv(f\"{stock}_pca.csv\")\n",
    "    \n",
    "    # 数据划分\n",
    "    train_size = int(len(pca_data) * 0.6)\n",
    "    val_size = int(len(pca_data) * 0.2)\n",
    "    test_size = len(pca_data) - train_size - val_size\n",
    "    train_data = pca_data[:train_size]\n",
    "    val_data = pca_data[train_size:train_size + val_size]\n",
    "    test_data = pca_data[train_size + val_size:]\n",
    "    \n",
    "    # 创建降噪后的数据框\n",
    "    denoised_train = train_data.copy()\n",
    "    denoised_val = val_data.copy()\n",
    "    denoised_test = test_data.copy()\n",
    "    \n",
    "    # 遍历每一列（PCA 主成分），对所有列进行降噪\n",
    "    for column in pca_data.columns:\n",
    "        denoised_train[column] = train_data[column]\n",
    "        denoised_val[column] = val_data[column]\n",
    "        denoised_test[column] = test_data[column]\n",
    "    \n",
    "    # 加载标签（假设有目标列 'target'）\n",
    "    y_train = train_data[\"target\"]\n",
    "    y_val = val_data[\"target\"]\n",
    "    y_test = test_data[\"target\"]\n",
    "    \n",
    "    # 标签编码（针对二分类问题）\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)  # 0 和 1\n",
    "    y_val_enc = le.transform(y_val)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "\n",
    "    # 定义参数网格，包括小波基\n",
    "    param_grid = {\n",
    "        \"wavelet\": wavelet_base,                 # 小波基\n",
    "        \"xgb__n_estimators\": [100, 200],         # 弱学习器数量\n",
    "        \"xgb__max_depth\": [3, 5, 7],            # 树深度\n",
    "        \"xgb__learning_rate\": [0.01, 0.1, 0.3], # 学习率\n",
    "        \"xgb__subsample\": [0.6, 0.8, 1.0],      # 数据采样比例\n",
    "        \"xgb__colsample_bytree\": [0.6, 0.8],    # 特征采样比例\n",
    "        \"xgb__gamma\": [0, 0.1, 0.5]             # 分裂正则化参数\n",
    "    }\n",
    "\n",
    "    # 定义降噪与训练的自定义流程\n",
    "    def custom_pipeline(params):\n",
    "        wavelet = params[\"wavelet\"]\n",
    "        xgb_params = {key.split(\"__\")[1]: value for key, value in params.items() if key.startswith(\"xgb__\")}\n",
    "\n",
    "        # 对训练集、验证集和测试集降噪\n",
    "        for column in train_data.columns:\n",
    "            if column != \"target\":\n",
    "                denoised_train[column] = denoise_signal(train_data[column], wavelet, level=5)\n",
    "                denoised_val[column] = denoise_signal(val_data[column], wavelet, level=5)\n",
    "                denoised_test[column] = denoise_signal(test_data[column], wavelet, level=5)\n",
    "        \n",
    "        # 初始化 XGBoost 模型\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            **xgb_params\n",
    "        )\n",
    "\n",
    "        # 模型训练\n",
    "        xgb_model.fit(\n",
    "            denoised_train.drop(columns=[\"target\"]), y_train_enc,\n",
    "            eval_set=[(denoised_val.drop(columns=[\"target\"]), y_val_enc)],\n",
    "            #early_stopping_rounds=10,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # 返回验证集的准确率\n",
    "        val_pred = xgb_model.predict(denoised_val.drop(columns=[\"target\"]))\n",
    "        acc = accuracy_score(y_val_enc, val_pred)\n",
    "        return acc\n",
    "\n",
    "    # 网格搜索实现\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    # 遍历所有参数组合\n",
    "    from itertools import product\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    for param_comb in product(*values):\n",
    "        params = dict(zip(keys, param_comb))\n",
    "        score = custom_pipeline(params)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "\n",
    "    # 输出最佳结果\n",
    "    print(f\"Stock: {stock}\")\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Accuracy (Validation):\", best_score)\n",
    "\n",
    "    # 使用最佳参数训练最终模型\n",
    "    best_wavelet = best_params[\"wavelet\"]\n",
    "    best_xgb_params = {key.split(\"__\")[1]: value for key, value in best_params.items() if key.startswith(\"xgb__\")}\n",
    "\n",
    "    for column in train_data.columns:\n",
    "        if column != \"target\":\n",
    "            denoised_train[column] = denoise_signal(train_data[column], best_wavelet, level=5)\n",
    "            denoised_test[column] = denoise_signal(test_data[column], best_wavelet, level=5)\n",
    "\n",
    "    final_model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        **best_xgb_params\n",
    "    )\n",
    "    final_model.fit(\n",
    "        denoised_train.drop(columns=[\"target\"]), y_train_enc\n",
    "    )\n",
    "    y_test_pred = final_model.predict(denoised_test.drop(columns=[\"target\"]))\n",
    "    test_acc = accuracy_score(y_test_enc, y_test_pred)\n",
    "    print(f\"Test Accuracy: {test_acc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12abdbb6-ef71-4b28-90d0-989e6077cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
